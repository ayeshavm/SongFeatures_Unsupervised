{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Determine Flesch-Kincaid Grade Level\n",
    "\n",
    "\n",
    "From [wikipedia](https://en.wikipedia.org/wiki/Flesch–Kincaid_readability_tests):\n",
    "\n",
    "The Flesch–Kincaid readability tests are readability tests designed to indicate how difficult a passage in English is to understand. There are two tests, the Flesch Reading Ease, and the Flesch–Kincaid Grade Level. Although they use the same core measures (word length and sentence length), they have different weighting factors.\n",
    "\n",
    "The results of the two tests correlate approximately inversely: a text with a comparatively high score on the Reading Ease test should have a lower score on the Grade-Level test. \n",
    "\n",
    "\n",
    "These readability tests are used extensively in the field of education. The \"Flesch–Kincaid Grade Level Formula\" instead presents a score as a U.S. grade level, making it easier for teachers, parents, librarians, and others to judge the readability level of various books and texts. It can also mean the number of years of education generally required to understand this text, relevant when the formula results in a number greater than 10. The grade level is calculated with the following formula:\n",
    "\n",
    "$$0.39 (\\frac{total words}{total sentences}) + 11.8 (\\frac{total syllables}{total words}) - 15.59 $$\n",
    "\n",
    "\n",
    "\n",
    "The result is a number that corresponds with a U.S. grade level. The sentence, \"The Australian platypus is seemingly a hybrid of a mammal and reptilian creature\" is an 11.3 as it has 24 syllables and 13 words. The different weighting factors for words per sentence and syllables per word in each scoring system mean that the two schemes are not directly comparable and cannot be converted. The grade level formula emphasises sentence length over word length. By creating one-word strings with hundreds of random characters, grade levels may be attained that are hundreds of times larger than high school completion in the United States. Due to the formula's construction, the score does not have an upper bound.\n",
    "\n",
    "The lowest grade level score in theory is −3.40, but there are few real passages in which every sentence consists of a single one-syllable word. Green Eggs and Ham by Dr. Seuss comes close, averaging 5.7 words per sentence and 1.02 syllables per word, with a grade level of −1.3. (Most of the 50 used words are monosyllabic; \"anywhere\", which occurs eight times, is the only exception.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Read Pre-Processed Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDF = pd.read_pickle('../data/msongs/out/Country_Preprocessed_1109.p')\n",
    "popDF = pd.read_pickle('../data/msongs/out/Pop_Preprocessed_1109.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'artist_id', 'tags', 'track_id', 'title', 'song_id', 'release',\n",
       "       'artist_mbid', 'artist_name', 'duration', 'artist_familiarity',\n",
       "       'artist_hotttnesss', 'year', 'track_7digitalid', 'shs_perf', 'shs_work',\n",
       "       'lyrics_text', 'spotifyURI', 'songFeatures', 'danceability', 'energy',\n",
       "       'key', 'loudness', 'mode', 'speechiness', 'acousticness',\n",
       "       'instrumentalness', 'liveness', 'valence', 'tempo', 'type', 'id', 'uri',\n",
       "       'track_href', 'analysis_url', 'duration_ms', 'time_signature', 'genre',\n",
       "       'country_count', 'pop_count', 'other_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'index', 'artist_id', 'tags', 'track_id', 'title', 'song_id',\n",
       "       'release', 'artist_mbid', 'artist_name', 'duration',\n",
       "       'artist_familiarity', 'artist_hotttnesss', 'year', 'track_7digitalid',\n",
       "       'shs_perf', 'shs_work', 'lyrics_text', 'spotifyURI', 'songFeatures',\n",
       "       'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n",
       "       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
       "       'type', 'id', 'uri', 'track_href', 'analysis_url', 'duration_ms',\n",
       "       'time_signature', 'genre', 'country_count', 'pop_count', 'other_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countryDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countryDF.drop('level_0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songsDF = pd.DataFrame(data=None, columns=popDF.columns)\n",
    "songsDF = popDF.append(countryDF, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23850"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(songsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for testing subset\n",
    "beyonce = songsDF[songsDF.artist_id == 'AR65K7A1187FB4DAA4'].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text Pre-processing:  Clean Lyrics to replace characters and remove verse tags\n",
    "\n",
    "* drop records where language = 'en'\n",
    "* remove tags i.e. [Chorus], [Verses], etc\n",
    "* remove extra lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import feedparser\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### language Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "r_lang = []\n",
    "for idx, row in songsDF.iterrows():\n",
    "    try:\n",
    "        r_lang.append(detect(row.lyrics_text))\n",
    "    except:\n",
    "        r_lang.append('not found')\n",
    "        print('error processing row {}'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songsDF['language'] = r_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no', 'sk', 'id', 'sv', 'sl', 'tl', 'pl', 'hr', 'ca', 'pt', 'fr', 'af', 'vi', 'cs', 'cy', 'et', 'sq', 'fi', 'ar', 'sw', 'fa', 'da', 'tr', 'not found', 'hu', 'nl', 'de', 'ro', 'it', 'lv', 'en', 'es', 'so'} \n",
      "\n",
      "no : 241\n",
      "sk : 1\n",
      "id : 33\n",
      "sv : 168\n",
      "sl : 1\n",
      "tl : 16\n",
      "pl : 16\n",
      "hr : 12\n",
      "ca : 25\n",
      "pt : 224\n",
      "fr : 239\n",
      "af : 7\n",
      "vi : 1\n",
      "cs : 1\n",
      "cy : 2\n",
      "et : 3\n",
      "sq : 1\n",
      "fi : 169\n",
      "ar : 2\n",
      "sw : 9\n",
      "fa : 1\n",
      "da : 26\n",
      "tr : 11\n",
      "not found : 14\n",
      "hu : 4\n",
      "nl : 93\n",
      "de : 368\n",
      "ro : 441\n",
      "it : 212\n",
      "lv : 3\n",
      "en : 20447\n",
      "es : 1043\n",
      "so : 16\n"
     ]
    }
   ],
   "source": [
    "#count number of records by language\n",
    "\n",
    "print(set(r_lang), '\\n')\n",
    "\n",
    "for lang in set(r_lang):\n",
    "    print(lang, ':', r_lang.count(lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter out languages other than english\n",
    "other_lang_idx = songsDF.index[songsDF['language'] != 'en']\n",
    "songsDF.drop(other_lang_idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Other Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nullidx = songsDF.index[songsDF.valence.isnull() == True] \n",
    "songsDF.drop(nullidx, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Lyrics and save it in the dataframe so lyrics processing in the future will no longer need this cleaning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lyrics_cleaned = []\n",
    "lyrics = ''\n",
    "for idx, row in songsDF.iterrows():\n",
    "    try:\n",
    "        lyrics = re.sub(r'[\\(\\[].*?[\\)\\]]', '.', row.lyrics_text)\n",
    "        lyrics = os.linesep.join([s for s in lyrics.splitlines() if s])\n",
    "    except:\n",
    "        #lyrics_cleaned.append('not found')\n",
    "        print('error processing row {}'.format(idx))\n",
    "    \n",
    "    lyrics_cleaned.append(lyrics)\n",
    "\n",
    "#remove empty lines\n",
    "\n",
    "#all_words = all_words.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songsDF['lyrics_clean'] = lyrics_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songsDF.to_pickle('../data/msongs/out/Songs_TextPreProcessed_1b.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>track_id</th>\n",
       "      <th>title</th>\n",
       "      <th>song_id</th>\n",
       "      <th>release</th>\n",
       "      <th>artist_mbid</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>duration</th>\n",
       "      <th>...</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>genre</th>\n",
       "      <th>country_count</th>\n",
       "      <th>pop_count</th>\n",
       "      <th>other_count</th>\n",
       "      <th>language</th>\n",
       "      <th>lyrics_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>AR009211187B989185</td>\n",
       "      <td>[(pop rock,), (pop,), (synthpop,), (reggae pop,)]</td>\n",
       "      <td>TRDBNUI128F933DE6E</td>\n",
       "      <td>I'm So Sorry</td>\n",
       "      <td>SOZCBYK12AB0180B4D</td>\n",
       "      <td>The Best Of Original British Lovers Rock Volum...</td>\n",
       "      <td>9dfe78a6-6d91-454e-9b95-9d7722cbc476</td>\n",
       "      <td>Carroll Thompson</td>\n",
       "      <td>260.91057</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/19mWMfe0fX4l...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/19mW...</td>\n",
       "      <td>426520.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>pop</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>Gabrielle\\nGabrielle\\nI'm So Glad\\nI can never...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>AR00FOZ1187FB5C9F3</td>\n",
       "      <td>[(synthpop,), (pop,), (pukkelpop,)]</td>\n",
       "      <td>TRWTSUW12903CD2DEA</td>\n",
       "      <td>Stay The Same</td>\n",
       "      <td>SOBGFYV12AB018DEAD</td>\n",
       "      <td>Stay The Same</td>\n",
       "      <td>92337972-f0c5-4ebd-be8c-f6b23d596ae1</td>\n",
       "      <td>autoKratz</td>\n",
       "      <td>404.97587</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/1wZY4LPJsABn...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/1wZY...</td>\n",
       "      <td>283333.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>pop</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>We can't always stay the same, but we all keep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>AR00FOZ1187FB5C9F3</td>\n",
       "      <td>[(synthpop,), (pop,), (pukkelpop,)]</td>\n",
       "      <td>TRNZCDV128F92F00A1</td>\n",
       "      <td>Stay The Same</td>\n",
       "      <td>SOYUZXU12A58A78201</td>\n",
       "      <td>Animal</td>\n",
       "      <td>92337972-f0c5-4ebd-be8c-f6b23d596ae1</td>\n",
       "      <td>autoKratz</td>\n",
       "      <td>311.06567</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/1gZ4TP1pQwRD...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/1gZ4...</td>\n",
       "      <td>311200.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>pop</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>We can't always stay the same, but we all keep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>AR00FOZ1187FB5C9F3</td>\n",
       "      <td>[(synthpop,), (pop,), (pukkelpop,)]</td>\n",
       "      <td>TRPGUDC12903CD2DEC</td>\n",
       "      <td>Stay The Same</td>\n",
       "      <td>SOBZAGY12AB018E2A5</td>\n",
       "      <td>Stay The Same</td>\n",
       "      <td>92337972-f0c5-4ebd-be8c-f6b23d596ae1</td>\n",
       "      <td>autoKratz</td>\n",
       "      <td>277.73342</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/26wPSNT05P58...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/26wP...</td>\n",
       "      <td>291000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>pop</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>We can't always stay the same, but we all keep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>AR00FOZ1187FB5C9F3</td>\n",
       "      <td>[(synthpop,), (pop,), (pukkelpop,)]</td>\n",
       "      <td>TRTMPTG128F92F00A0</td>\n",
       "      <td>Always More</td>\n",
       "      <td>SOUQQXB12A8C140687</td>\n",
       "      <td>Animal</td>\n",
       "      <td>92337972-f0c5-4ebd-be8c-f6b23d596ae1</td>\n",
       "      <td>autoKratz</td>\n",
       "      <td>255.26812</td>\n",
       "      <td>...</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/1gZ4TP1pQwRD...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/1gZ4...</td>\n",
       "      <td>311200.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>pop</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>Faith I'm sure, there's something wanting but ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index           artist_id  \\\n",
       "0    0.0  AR009211187B989185   \n",
       "1    1.0  AR00FOZ1187FB5C9F3   \n",
       "2    2.0  AR00FOZ1187FB5C9F3   \n",
       "3    3.0  AR00FOZ1187FB5C9F3   \n",
       "4    4.0  AR00FOZ1187FB5C9F3   \n",
       "\n",
       "                                                tags            track_id  \\\n",
       "0  [(pop rock,), (pop,), (synthpop,), (reggae pop,)]  TRDBNUI128F933DE6E   \n",
       "1                [(synthpop,), (pop,), (pukkelpop,)]  TRWTSUW12903CD2DEA   \n",
       "2                [(synthpop,), (pop,), (pukkelpop,)]  TRNZCDV128F92F00A1   \n",
       "3                [(synthpop,), (pop,), (pukkelpop,)]  TRPGUDC12903CD2DEC   \n",
       "4                [(synthpop,), (pop,), (pukkelpop,)]  TRTMPTG128F92F00A0   \n",
       "\n",
       "           title             song_id  \\\n",
       "0   I'm So Sorry  SOZCBYK12AB0180B4D   \n",
       "1  Stay The Same  SOBGFYV12AB018DEAD   \n",
       "2  Stay The Same  SOYUZXU12A58A78201   \n",
       "3  Stay The Same  SOBZAGY12AB018E2A5   \n",
       "4    Always More  SOUQQXB12A8C140687   \n",
       "\n",
       "                                             release  \\\n",
       "0  The Best Of Original British Lovers Rock Volum...   \n",
       "1                                      Stay The Same   \n",
       "2                                             Animal   \n",
       "3                                      Stay The Same   \n",
       "4                                             Animal   \n",
       "\n",
       "                            artist_mbid       artist_name   duration  \\\n",
       "0  9dfe78a6-6d91-454e-9b95-9d7722cbc476  Carroll Thompson  260.91057   \n",
       "1  92337972-f0c5-4ebd-be8c-f6b23d596ae1         autoKratz  404.97587   \n",
       "2  92337972-f0c5-4ebd-be8c-f6b23d596ae1         autoKratz  311.06567   \n",
       "3  92337972-f0c5-4ebd-be8c-f6b23d596ae1         autoKratz  277.73342   \n",
       "4  92337972-f0c5-4ebd-be8c-f6b23d596ae1         autoKratz  255.26812   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "\n",
       "                                          track_href  \\\n",
       "0  https://api.spotify.com/v1/tracks/19mWMfe0fX4l...   \n",
       "1  https://api.spotify.com/v1/tracks/1wZY4LPJsABn...   \n",
       "2  https://api.spotify.com/v1/tracks/1gZ4TP1pQwRD...   \n",
       "3  https://api.spotify.com/v1/tracks/26wPSNT05P58...   \n",
       "4  https://api.spotify.com/v1/tracks/1gZ4TP1pQwRD...   \n",
       "\n",
       "                                        analysis_url duration_ms  \\\n",
       "0  https://api.spotify.com/v1/audio-analysis/19mW...    426520.0   \n",
       "1  https://api.spotify.com/v1/audio-analysis/1wZY...    283333.0   \n",
       "2  https://api.spotify.com/v1/audio-analysis/1gZ4...    311200.0   \n",
       "3  https://api.spotify.com/v1/audio-analysis/26wP...    291000.0   \n",
       "4  https://api.spotify.com/v1/audio-analysis/1gZ4...    311200.0   \n",
       "\n",
       "  time_signature genre country_count pop_count other_count language  \\\n",
       "0            4.0   pop           0.0       4.0         0.0       en   \n",
       "1            4.0   pop           0.0       3.0         0.0       en   \n",
       "2            4.0   pop           0.0       3.0         0.0       en   \n",
       "3            4.0   pop           0.0       3.0         0.0       en   \n",
       "4            4.0   pop           0.0       3.0         0.0       en   \n",
       "\n",
       "                                        lyrics_clean  \n",
       "0  Gabrielle\\nGabrielle\\nI'm So Glad\\nI can never...  \n",
       "1  We can't always stay the same, but we all keep...  \n",
       "2  We can't always stay the same, but we all keep...  \n",
       "3  We can't always stay the same, but we all keep...  \n",
       "4  Faith I'm sure, there's something wanting but ...  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These sections are just to view and analyze lyrics text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_lyrics = beyonce.loc[3,'lyrics_text']\n",
    "lyrics = beyonce.loc[3,'lyrics_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean the lyrics to replace characters and remove verse tags i.e. '[Verse 1]'\n",
    "\n",
    "lyrics = re.sub(r'[\\(\\[].*?[\\)\\]]', '', lyrics)\n",
    "\n",
    "#remove empty lines\n",
    "lyrics = os.linesep.join([s for s in lyrics.splitlines() if s])\n",
    "#all_words = all_words.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Verse 1]\\nDiamonds used to be coal\\nLook young cause they got soul\\nThat\\'s why they\\'re beautiful...\\nAnd my heart used to be cold\\n\\'til your hands laid on my soul\\nBaby, that\\'s why you\\'re beautiful...\\n\\n[Chorus]\\nI\\'m not wondering why\\nThe sky\\'s blue; that\\'s not my business\\nAll I know is I\\nLook up and tell myself\\n\"Be patient, love. That could be us.\"\\n\\n[Verse 2]\\nLovers used to make love\\nAnd died just to give us\\nTheir piece of the beautiful\\nRemember when we made love?\\nLove...\\nWasn\\'t it beautiful?\\n\\n[Chorus]\\nDon\\'t ask me why\\nThe sky\\'s blue; that\\'s not my business\\nAll I know is I....\\nLook up and tell myself\\n\"Be patient, love. That could be us.\"\\n\\n[Bridge]\\nDiamonds used to be coal\\nLook young cause they got soul\\nAnd my heart used to be cold\\n\\'Til your hands laid on my soul\\nSomebody\\'s got to stay deep in love\\nThat could be us...\\nThat\\'s why we\\'re beautiful\\nThat\\'s why you\\'re beautiful\\nOoh\\nWhy, why\\nThat\\'s why you\\'re beautiful\\nThat\\'s why you\\'re beautiful\\nThat\\'s why you\\'re beautiful'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verse 1]\n",
      "Diamonds used to be coal\n",
      "Look young cause they got soul\n",
      "That's why they're beautiful...\n",
      "And my heart used to be cold\n",
      "'til your hands laid on my soul\n",
      "Baby, that's why you're beautiful...\n",
      "\n",
      "[Chorus]\n",
      "I'm not wondering why\n",
      "The sky's blue; that's not my business\n",
      "All I know is I\n",
      "Look up and tell myself\n",
      "\"Be patient, love. That could be us.\"\n",
      "\n",
      "[Verse 2]\n",
      "Lovers used to make love\n",
      "And died just to give us\n",
      "Their piece of the beautiful\n",
      "Remember when we made love?\n",
      "Love...\n",
      "Wasn't it beautiful?\n",
      "\n",
      "[Chorus]\n",
      "Don't ask me why\n",
      "The sky's blue; that's not my business\n",
      "All I know is I....\n",
      "Look up and tell myself\n",
      "\"Be patient, love. That could be us.\"\n",
      "\n",
      "[Bridge]\n",
      "Diamonds used to be coal\n",
      "Look young cause they got soul\n",
      "And my heart used to be cold\n",
      "'Til your hands laid on my soul\n",
      "Somebody's got to stay deep in love\n",
      "That could be us...\n",
      "That's why we're beautiful\n",
      "That's why you're beautiful\n",
      "Ooh\n",
      "Why, why\n",
      "That's why you're beautiful\n",
      "That's why you're beautiful\n",
      "That's why you're beautiful\n"
     ]
    }
   ],
   "source": [
    "print(original_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Diamonds used to be coal\\nLook young cause they got soul\\nThat\\'s why they\\'re beautiful...\\nAnd my heart used to be cold\\n\\'til your hands laid on my soul\\nBaby, that\\'s why you\\'re beautiful...\\nI\\'m not wondering why\\nThe sky\\'s blue; that\\'s not my business\\nAll I know is I\\nLook up and tell myself\\n\"Be patient, love. That could be us.\"\\nLovers used to make love\\nAnd died just to give us\\nTheir piece of the beautiful\\nRemember when we made love?\\nLove...\\nWasn\\'t it beautiful?\\nDon\\'t ask me why\\nThe sky\\'s blue; that\\'s not my business\\nAll I know is I....\\nLook up and tell myself\\n\"Be patient, love. That could be us.\"\\nDiamonds used to be coal\\nLook young cause they got soul\\nAnd my heart used to be cold\\n\\'Til your hands laid on my soul\\nSomebody\\'s got to stay deep in love\\nThat could be us...\\nThat\\'s why we\\'re beautiful\\nThat\\'s why you\\'re beautiful\\nOoh\\nWhy, why\\nThat\\'s why you\\'re beautiful\\nThat\\'s why you\\'re beautiful\\nThat\\'s why you\\'re beautiful'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diamonds used to be coal\n",
      "Look young cause they got soul\n",
      "That's why they're beautiful...\n",
      "And my heart used to be cold\n",
      "'til your hands laid on my soul\n",
      "Baby, that's why you're beautiful...\n",
      "I'm not wondering why\n",
      "The sky's blue; that's not my business\n",
      "All I know is I\n",
      "Look up and tell myself\n",
      "\"Be patient, love. That could be us.\"\n",
      "Lovers used to make love\n",
      "And died just to give us\n",
      "Their piece of the beautiful\n",
      "Remember when we made love?\n",
      "Love...\n",
      "Wasn't it beautiful?\n",
      "Don't ask me why\n",
      "The sky's blue; that's not my business\n",
      "All I know is I....\n",
      "Look up and tell myself\n",
      "\"Be patient, love. That could be us.\"\n",
      "Diamonds used to be coal\n",
      "Look young cause they got soul\n",
      "And my heart used to be cold\n",
      "'Til your hands laid on my soul\n",
      "Somebody's got to stay deep in love\n",
      "That could be us...\n",
      "That's why we're beautiful\n",
      "That's why you're beautiful\n",
      "Ooh\n",
      "Why, why\n",
      "That's why you're beautiful\n",
      "That's why you're beautiful\n",
      "That's why you're beautiful\n"
     ]
    }
   ],
   "source": [
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Compute for Flesch Kincaid Readability Grade \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textstat.textstat import textstatistics, easy_word_set, legacy_round\n",
    "from nltk.tokenize import TreebankWordTokenizer, WordPunctTokenizer, WhitespaceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import curses \n",
    "from curses.ascii import isdigit \n",
    "import nltk \n",
    "from nltk.corpus import cmudict \n",
    "\n",
    "d = cmudict.dict() \n",
    "\n",
    "def nsyl(text):\n",
    "    numsyl = 0\n",
    "    #for word in nltk.word_tokenize(text):\n",
    "    for word in WhitespaceTokenizer().tokenize(text):\n",
    "        try:\n",
    "            syllen = [len(list(y for y in x if isdigit(y[-1]))) for x in d[word.lower()]]\n",
    "        except:\n",
    "            syllen = [1]\n",
    "       \n",
    "        numsyl += syllen[0]\n",
    "    \n",
    "    return numsyl  \n",
    "\n",
    "def break_sentences_nltk(text):\n",
    "    sent_tokenize_list = sent_tokenize(lyrics)\n",
    "    return sent_tokenize_list\n",
    "\n",
    "def word_count_nltk(text):\n",
    "    sentences = break_sentences_nltk(text)\n",
    "    words = 0\n",
    "    for sentence in sentences:\n",
    "        #words += len([token for token in nltk.word_tokenize(sentence)])\n",
    "        words += len([token for token in WhitespaceTokenizer().tokenize(sentence)])\n",
    "    return words\n",
    "\n",
    "def sentence_count_nltk(text):\n",
    "    sentences = break_sentences_nltk(text)\n",
    "    out_sentences = list(sentences)\n",
    "    return len(out_sentences)\n",
    "\n",
    "def avg_syllables_per_word_nltk(text):\n",
    "    syllable = nsyl(text)\n",
    "    #print('syllable', syllable)\n",
    "    words = word_count_nltk(text)\n",
    "    #print('words', words)\n",
    "    ASPW = float(syllable) / float(words)\n",
    "    #print('ASPW', ASPW)\n",
    "    return legacy_round(ASPW, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def break_sentences(text):\n",
    "    nlp = spacy.load('en')\n",
    "    doc = nlp(text)\n",
    "    return doc.sents\n",
    "\n",
    "def word_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "    words = 0\n",
    "    for sentence in sentences:\n",
    "        words += len([token for token in sentence])\n",
    "    return words\n",
    "\n",
    "def sentence_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "    out_sentences = list(sentences)\n",
    "    return len(out_sentences)\n",
    "\n",
    "def avg_sentence_length(text):\n",
    "    words = word_count(text)\n",
    "    sentences = sentence_count(text)\n",
    "    average_sentence_length = float(words / sentences)\n",
    "    return average_sentence_length\n",
    "\n",
    "def syllables_count(word):\n",
    "    return textstatistics().syllable_count(word)\n",
    "\n",
    "def avg_syllables_per_word(text):\n",
    "    syllable = syllables_count(text)\n",
    "    words = word_count(text)\n",
    "    ASPW = float(syllable) / float(words)\n",
    "    return legacy_round(ASPW, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flesch_kincaid_reading_grade(lyrics):\n",
    "    FKRG = 0.0\n",
    "    \n",
    "    #lyrics = clean_lyrics_text(text)\n",
    "    try:\n",
    "        FKRG = float(0.39 * (word_count(lyrics)/sentence_count(lyrics) ))  + \\\n",
    "               float(11.8 * avg_syllables_per_word(lyrics))\n",
    "    except:\n",
    "        FKRG = 0.0\n",
    "                    \n",
    "                     \n",
    "    return legacy_round(FKRG,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flesch_kincaid_reading_grade_nltk2(lyrics):\n",
    "    ''' this function computes the Flesch Kincaid Reading Grade for\n",
    "        sentence tokenizer it is using spacy.'''\n",
    "    FKRG = 0.0\n",
    "    sent_count = 0.0\n",
    "    try:\n",
    "        sent_count = sentence_count(lyrics)\n",
    "        FKRG = (float(0.39 * (word_count_nltk(lyrics)/sent_count ))  + \\\n",
    "               float(11.8 * avg_syllables_per_word_nltk(lyrics))) - 15.59\n",
    "    except:\n",
    "        FKRG = 0.0\n",
    "                    \n",
    "                     \n",
    "    return legacy_round(FKRG,2), sent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flesch_kincaid_reading_grade_nltk(lyrics):\n",
    "    ''' this function computes the Flesch Kincaid Reading Grade for\n",
    "        sentence tokenizer it is using nltk.'''\n",
    "    \n",
    "    FKRG = 0.0\n",
    "    try:\n",
    "        sent_count = sentence_count_nltk(lyrics)\n",
    "        FKRG = (float(0.39 * (word_count_nltk(lyrics)/sent_count ))  + \\\n",
    "               float(11.8 * avg_syllables_per_word_nltk(lyrics))) - 15.59\n",
    "    except:\n",
    "        FKRG = 0.0\n",
    "                    \n",
    "                     \n",
    "    return legacy_round(FKRG,2), sent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\nI jumped in the river and what did I see?\\nBlack-eyed angels swam with me\\nA moon full of stars and astral cars\\nAnd all the figures I used to see\\nAll my lovers were there with me\\nAll my past and futures\\nAnd we all went to heaven in a little row boat\\nThere was nothing to fear and nothing to doubt\\n.\\nI jumped into the river\\nBlack-eyed angels swam with me\\nA moon full of stars and astral cars\\nAnd all the figures I used to see\\nAll my lovers were there with me\\nAll my past and futures\\nAnd we all went to heaven in a little row boat\\nThere was nothing to fear and nothing to doubt\\nThere was nothing to fear and nothing to doubt\\nThere was nothing to fear and nothing to doubt'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lyrics = beyonce.loc[3,'lyrics_text']\n",
    "lyrics = clean_lyrics_text(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lyrics = df.loc[4,'lyrics_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62.96, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lyrics = beyonce.loc[3,'lyrics_text']\n",
    "#FKRG = flesch_kincaid_reading_grade(lyrics)\n",
    "FKRG_nltk, count = flesch_kincaid_reading_grade_nltk(lyrics)\n",
    "FKRG_nltk, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.81, 18)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FKRG_nltk, count = flesch_kincaid_reading_grade_nltk2(lyrics)\n",
    "FKRG_nltk, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tokens = nltk.word_tokenize(lyrics)\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Diamonds used to be coal\n",
      "\n",
      "2 Look young cause they got soul\n",
      "\n",
      "3 That's why they're beautiful...\n",
      "\n",
      "4 And my heart used to be cold\n",
      "\n",
      "5 'til your hands laid on my soul\n",
      "Baby\n",
      "6 , that's why you're beautiful...\n",
      "\n",
      "7 I'm not wondering why\n",
      "The sky's blue; that's not my business\n",
      "\n",
      "8 All I know is I\n",
      "Look up and tell myself\n",
      "\"Be patient, love.\n",
      "9 That could be us.\"\n",
      "\n",
      "10 Lovers used to make love\n",
      "And died just to give us\n",
      "Their piece of the beautiful\n",
      "Remember when we made love?\n",
      "\n",
      "11 Love...\n",
      "\n",
      "12 Wasn't it beautiful?\n",
      "\n",
      "13 Don't ask me why\n",
      "The sky's blue; that's not my business\n",
      "\n",
      "14 All I know is I....\n",
      "\n",
      "15 Look up and tell myself\n",
      "\"Be patient, love.\n",
      "16 That could be us.\"\n",
      "\n",
      "17 Diamonds used to be coal\n",
      "\n",
      "18 Look young cause they got soul\n",
      "\n",
      "19 And my heart used to be cold\n",
      "'\n",
      "20 Til your hands laid on my soul\n",
      "\n",
      "21 Somebody's got to stay deep in love\n",
      "\n",
      "22 That could be us...\n",
      "\n",
      "23 That's why we're beautiful\n",
      "\n",
      "24 That's why you're beautiful\n",
      "\n",
      "25 Ooh\n",
      "\n",
      "26 Why, why\n",
      "That's why you're beautiful\n",
      "\n",
      "27 That's why you're beautiful\n",
      "\n",
      "28 That's why you're beautiful\n",
      "total sentences 28\n"
     ]
    }
   ],
   "source": [
    "#spacy break sentences\n",
    "spacy_sent = break_sentences(lyrics)\n",
    "\n",
    "count = 0\n",
    "for sent in spacy_sent:\n",
    "    count += 1\n",
    "    print(count, sent)\n",
    "    \n",
    "print('total sentences', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare with nltk sent_tokenizer\n",
    "sent_tokenize_list = sent_tokenize(lyrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Diamonds used to be coal\n",
      "Look young cause they got soul\n",
      "That's why they're beautiful...\n",
      "And my heart used to be cold\n",
      "'til your hands laid on my soul\n",
      "Baby, that's why you're beautiful...\n",
      "2 I'm not wondering why\n",
      "The sky's blue; that's not my business\n",
      "All I know is I\n",
      "Look up and tell myself\n",
      "\"Be patient, love.\n",
      "3 That could be us.\"\n",
      "4 Lovers used to make love\n",
      "And died just to give us\n",
      "Their piece of the beautiful\n",
      "Remember when we made love?\n",
      "5 Love...\n",
      "Wasn't it beautiful?\n",
      "6 Don't ask me why\n",
      "The sky's blue; that's not my business\n",
      "All I know is I....\n",
      "Look up and tell myself\n",
      "\"Be patient, love.\n",
      "7 That could be us.\"\n",
      "8 Diamonds used to be coal\n",
      "Look young cause they got soul\n",
      "And my heart used to be cold\n",
      "'Til your hands laid on my soul\n",
      "Somebody's got to stay deep in love\n",
      "That could be us...\n",
      "That's why we're beautiful\n",
      "That's why you're beautiful\n",
      "Ooh\n",
      "Why, why\n",
      "That's why you're beautiful\n",
      "That's why you're beautiful\n",
      "That's why you're beautiful\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for sent in sent_tokenize_list:\n",
    "    #print(count, sent)\n",
    "    count += 1\n",
    "    print(count, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diamonds used to be coal\n",
      "Look young cause they got soul\n",
      "That's why they're beautiful...\n",
      "And my heart used to be cold\n",
      "'til your hands laid on my soul\n",
      "Baby, that's why you're beautiful...\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare various tokenizers\n",
    "from nltk.tokenize import TreebankWordTokenizer, WordPunctTokenizer, WhitespaceTokenizer\n",
    "\n",
    "sample_treebank_ = TreebankWordTokenizer().tokenize(lyrics)\n",
    "sample_wordpunct_ = WordPunctTokenizer().tokenize(lyrics)\n",
    "sample_wspace_ = WhitespaceTokenizer().tokenize(lyrics)\n",
    "sample_word_ = nltk.word_tokenize(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we are computing for syllables and the number of words for the Flesch-Kincaid Grade the Whitespace tokenizer will be used to keep contractions as one word/token to prevent counting unnecessary tokens as a word, which affects the Flesch Kincaid Reading grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Diamonds', 'used', 'to', 'be', 'coal', 'Look', 'young', 'cause', 'they', 'got', 'soul', 'That', \"'s\", 'why', 'they', \"'re\", 'beautiful', '...', 'And', 'my', 'heart', 'used', 'to', 'be', 'cold', \"'til\", 'your', 'hands', 'laid', 'on', 'my', 'soul', 'Baby', ',', 'that', \"'s\", 'why', 'you', \"'re\", 'beautiful', '...', 'I', \"'m\", 'not', 'wondering', 'why', 'The', 'sky', \"'s\", 'blue', ';', 'that', \"'s\", 'not', 'my', 'business', 'All', 'I', 'know', 'is', 'I', 'Look', 'up', 'and', 'tell', 'myself', \"''\", 'Be', 'patient', ',', 'love.', 'That', 'could', 'be', 'us.', \"''\", 'Lovers', 'used', 'to', 'make', 'love', 'And', 'died', 'just', 'to', 'give', 'us', 'Their', 'piece', 'of', 'the', 'beautiful', 'Remember', 'when', 'we', 'made', 'love', '?', 'Love', '...', 'Was', \"n't\", 'it', 'beautiful', '?', 'Do', \"n't\", 'ask', 'me', 'why', 'The', 'sky', \"'s\", 'blue', ';', 'that', \"'s\", 'not', 'my', 'business', 'All', 'I', 'know', 'is', 'I', '...', '.', 'Look', 'up', 'and', 'tell', 'myself', \"''\", 'Be', 'patient', ',', 'love.', 'That', 'could', 'be', 'us.', \"''\", 'Diamonds', 'used', 'to', 'be', 'coal', 'Look', 'young', 'cause', 'they', 'got', 'soul', 'And', 'my', 'heart', 'used', 'to', 'be', 'cold', \"'Til\", 'your', 'hands', 'laid', 'on', 'my', 'soul', 'Somebody', \"'s\", 'got', 'to', 'stay', 'deep', 'in', 'love', 'That', 'could', 'be', 'us', '...', 'That', \"'s\", 'why', 'we', \"'re\", 'beautiful', 'That', \"'s\", 'why', 'you', \"'re\", 'beautiful', 'Ooh', 'Why', ',', 'why', 'That', \"'s\", 'why', 'you', \"'re\", 'beautiful', 'That', \"'s\", 'why', 'you', \"'re\", 'beautiful', 'That', \"'s\", 'why', 'you', \"'re\", 'beautiful']\n",
      "Treebank Tokenizer found 214 tokens\n",
      "\n",
      "['Diamonds', 'used', 'to', 'be', 'coal', 'Look', 'young', 'cause', 'they', 'got', 'soul', 'That', \"'\", 's', 'why', 'they', \"'\", 're', 'beautiful', '...', 'And', 'my', 'heart', 'used', 'to', 'be', 'cold', \"'\", 'til', 'your', 'hands', 'laid', 'on', 'my', 'soul', 'Baby', ',', 'that', \"'\", 's', 'why', 'you', \"'\", 're', 'beautiful', '...', 'I', \"'\", 'm', 'not', 'wondering', 'why', 'The', 'sky', \"'\", 's', 'blue', ';', 'that', \"'\", 's', 'not', 'my', 'business', 'All', 'I', 'know', 'is', 'I', 'Look', 'up', 'and', 'tell', 'myself', '\"', 'Be', 'patient', ',', 'love', '.', 'That', 'could', 'be', 'us', '.\"', 'Lovers', 'used', 'to', 'make', 'love', 'And', 'died', 'just', 'to', 'give', 'us', 'Their', 'piece', 'of', 'the', 'beautiful', 'Remember', 'when', 'we', 'made', 'love', '?', 'Love', '...', 'Wasn', \"'\", 't', 'it', 'beautiful', '?', 'Don', \"'\", 't', 'ask', 'me', 'why', 'The', 'sky', \"'\", 's', 'blue', ';', 'that', \"'\", 's', 'not', 'my', 'business', 'All', 'I', 'know', 'is', 'I', '....', 'Look', 'up', 'and', 'tell', 'myself', '\"', 'Be', 'patient', ',', 'love', '.', 'That', 'could', 'be', 'us', '.\"', 'Diamonds', 'used', 'to', 'be', 'coal', 'Look', 'young', 'cause', 'they', 'got', 'soul', 'And', 'my', 'heart', 'used', 'to', 'be', 'cold', \"'\", 'Til', 'your', 'hands', 'laid', 'on', 'my', 'soul', 'Somebody', \"'\", 's', 'got', 'to', 'stay', 'deep', 'in', 'love', 'That', 'could', 'be', 'us', '...', 'That', \"'\", 's', 'why', 'we', \"'\", 're', 'beautiful', 'That', \"'\", 's', 'why', 'you', \"'\", 're', 'beautiful', 'Ooh', 'Why', ',', 'why', 'That', \"'\", 's', 'why', 'you', \"'\", 're', 'beautiful', 'That', \"'\", 's', 'why', 'you', \"'\", 're', 'beautiful', 'That', \"'\", 's', 'why', 'you', \"'\", 're', 'beautiful']\n",
      "WordPunct Tokenizer found 239 tokens\n",
      "\n",
      "['Diamonds', 'used', 'to', 'be', 'coal', 'Look', 'young', 'cause', 'they', 'got', 'soul', \"That's\", 'why', \"they're\", 'beautiful...', 'And', 'my', 'heart', 'used', 'to', 'be', 'cold', \"'til\", 'your', 'hands', 'laid', 'on', 'my', 'soul', 'Baby,', \"that's\", 'why', \"you're\", 'beautiful...', \"I'm\", 'not', 'wondering', 'why', 'The', \"sky's\", 'blue;', \"that's\", 'not', 'my', 'business', 'All', 'I', 'know', 'is', 'I', 'Look', 'up', 'and', 'tell', 'myself', '\"Be', 'patient,', 'love.', 'That', 'could', 'be', 'us.\"', 'Lovers', 'used', 'to', 'make', 'love', 'And', 'died', 'just', 'to', 'give', 'us', 'Their', 'piece', 'of', 'the', 'beautiful', 'Remember', 'when', 'we', 'made', 'love?', 'Love...', \"Wasn't\", 'it', 'beautiful?', \"Don't\", 'ask', 'me', 'why', 'The', \"sky's\", 'blue;', \"that's\", 'not', 'my', 'business', 'All', 'I', 'know', 'is', 'I....', 'Look', 'up', 'and', 'tell', 'myself', '\"Be', 'patient,', 'love.', 'That', 'could', 'be', 'us.\"', 'Diamonds', 'used', 'to', 'be', 'coal', 'Look', 'young', 'cause', 'they', 'got', 'soul', 'And', 'my', 'heart', 'used', 'to', 'be', 'cold', \"'Til\", 'your', 'hands', 'laid', 'on', 'my', 'soul', \"Somebody's\", 'got', 'to', 'stay', 'deep', 'in', 'love', 'That', 'could', 'be', 'us...', \"That's\", 'why', \"we're\", 'beautiful', \"That's\", 'why', \"you're\", 'beautiful', 'Ooh', 'Why,', 'why', \"That's\", 'why', \"you're\", 'beautiful', \"That's\", 'why', \"you're\", 'beautiful', \"That's\", 'why', \"you're\", 'beautiful']\n",
      "Whitespace Tokenizer found 174 tokens\n",
      "\n",
      "['Diamonds', 'used', 'to', 'be', 'coal', 'Look', 'young', 'cause', 'they', 'got', 'soul', 'That', \"'s\", 'why', 'they', \"'re\", 'beautiful', '...', 'And', 'my', 'heart', 'used', 'to', 'be', 'cold', \"'til\", 'your', 'hands', 'laid', 'on', 'my', 'soul', 'Baby', ',', 'that', \"'s\", 'why', 'you', \"'re\", 'beautiful', '...', 'I', \"'m\", 'not', 'wondering', 'why', 'The', 'sky', \"'s\", 'blue', ';', 'that', \"'s\", 'not', 'my', 'business', 'All', 'I', 'know', 'is', 'I', 'Look', 'up', 'and', 'tell', 'myself', \"''\", 'Be', 'patient', ',', 'love', '.', 'That', 'could', 'be', 'us', '.', \"''\", 'Lovers', 'used', 'to', 'make', 'love', 'And', 'died', 'just', 'to', 'give', 'us', 'Their', 'piece', 'of', 'the', 'beautiful', 'Remember', 'when', 'we', 'made', 'love', '?', 'Love', '...', 'Was', \"n't\", 'it', 'beautiful', '?', 'Do', \"n't\", 'ask', 'me', 'why', 'The', 'sky', \"'s\", 'blue', ';', 'that', \"'s\", 'not', 'my', 'business', 'All', 'I', 'know', 'is', 'I', '...', '.', 'Look', 'up', 'and', 'tell', 'myself', \"''\", 'Be', 'patient', ',', 'love', '.', 'That', 'could', 'be', 'us', '.', \"''\", 'Diamonds', 'used', 'to', 'be', 'coal', 'Look', 'young', 'cause', 'they', 'got', 'soul', 'And', 'my', 'heart', 'used', 'to', 'be', 'cold', \"'Til\", 'your', 'hands', 'laid', 'on', 'my', 'soul', 'Somebody', \"'s\", 'got', 'to', 'stay', 'deep', 'in', 'love', 'That', 'could', 'be', 'us', '...', 'That', \"'s\", 'why', 'we', \"'re\", 'beautiful', 'That', \"'s\", 'why', 'you', \"'re\", 'beautiful', 'Ooh', 'Why', ',', 'why', 'That', \"'s\", 'why', 'you', \"'re\", 'beautiful', 'That', \"'s\", 'why', 'you', \"'re\", 'beautiful', 'That', \"'s\", 'why', 'you', \"'re\", 'beautiful']\n",
      "nltk.word_tokenize Tokenizer found 218 tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sample_treebank_)\n",
    "print('Treebank Tokenizer found %d tokens\\n' % len(sample_treebank_))\n",
    "\n",
    "print(sample_wordpunct_)\n",
    "print('WordPunct Tokenizer found %d tokens\\n' % len(sample_wordpunct_))\n",
    "\n",
    "print(sample_wspace_)\n",
    "print('Whitespace Tokenizer found %d tokens\\n' % len(sample_wspace_))\n",
    "\n",
    "print(sample_word_)\n",
    "print('nltk.word_tokenize Tokenizer found %d tokens\\n' % len(sample_word_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "words = word_count_nltk(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "words = word_count(lyrics)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#beyonce.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "218\n",
      "8\n",
      "0.7\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#lyrics = beyonce.loc[3,'lyrics_text']\n",
    "#FKRG = flesch_kincaid_reading_grade(lyrics)\n",
    "FKRG_nltk = flesch_kincaid_reading_grade_nltk(lyrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.78 0.0\n"
     ]
    }
   ],
   "source": [
    "print(FKRG, FKRG_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test = 'lover'\n",
    "print(syllables_count(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actually\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(syllables_in_word('actually'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create FK_grade column by computing Flesch-Kincaid reading grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songsDF.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populate_FKRG(df):\n",
    "    \n",
    "    counter = 0\n",
    "    for i, row in enumerate(df[20000:].itertuples(), start=20000):\n",
    "    \n",
    "        lyrics = row.lyrics_clean\n",
    "        FKRG1, sent1 = flesch_kincaid_reading_grade_nltk(lyrics) #using sent_tokenize\n",
    "        FKRG2, sent2 = flesch_kincaid_reading_grade_nltk2(lyrics) #using spacy\n",
    "        \n",
    "        #FKRG = flesch_kincaid_reading_grade_nltk(lyrics)\n",
    "        df.set_value(i,'FKRG1', FKRG1)\n",
    "        df.set_value(i,'fkrg_sent1', sent1)\n",
    "        df.set_value(i,'FKRG2', FKRG2)\n",
    "        df.set_value(i,'fkrg_sent2', sent2)\n",
    "    \n",
    "        counter += 1\n",
    "        if counter % 500 == 0:\n",
    "            print('processing row ', i, row.artist_name)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songsDF = populate_FKRG(songsDF) #start 430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songsDF.to_pickle('../data/msongs/out/Songs_NLP_allFeatures_1b.p')\n",
    "#dfPop.to_pickle('../data/msongs/out/Pop_NLP_allFeatures.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
